![img](https://img-blog.csdn.net/20170315235146263?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjQ1Mzg0Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

## 技术架构 

我们淘淘商城的**技术架构**如下图所示，可以看到表现层与服务层之间加了一个服务中间件Dubbo，这样做的目的是加快表现层与服务层之间的交互速度，服务层与数据库之间加redis，可以大大提高重复性查询的效率，持久层是一个mysql集群，由MyCat数据库中间件来管理，MyCat相当于这么多mysql数据库的一个抽象，我们操作数据库直接面向的将是MyCat而不是具体的某个mysql，是由MyCat来替我们完成相关操作的。Solr服务是专门用来处理查询的，主要是通过建立索引库来实现的。消息队列则是专门处理各个模块之间的消息的。



## 淘淘商城的Maven目录结构

![img](https://img-blog.csdn.net/20170318113839914?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjQ1Mzg0Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

 pom工程一般都是父工程，管理jar包的版本、maven插件的版本、统一的依赖管理，是一个聚合工程。

​     jar工程，很显然就是pom工程的子工程，由pom工程来管理。

​     war工程，是web工程，可以直接放到tomcat下运行的工程。

## 为什么要使用tomcat插件来启动web工程？

       这是因为在互联网项目中，动辄十几个甚至好几十个web工程，如果按照传统的添加tomcat服务器的方式来启动的话（如下图所示），那么我们为了避免端口冲突，每增加一个web工程都要修改三个端口，非常麻烦。而tomcat插件启动则只需改一个端口即可，显然简单了很多。

##   Dubbo架构发展路线图

![img](https://img-blog.csdn.net/20170325172148134?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjQ1Mzg0Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

         从上图可以看到，发展经历了四个阶段：

第一阶段：单一应用架构

      当网站流量很小时，只需一个应用，将所有功能都部署在一起，以减少部署节点和成本。此时，用于简化增删改查工作量的数据访问框架（ORM）是关键。其中1~10的意思是，当一个tomcat服务无法满足要求时，我们可以增加部署tomcat的数量并用反向代理来做负载均衡。由于不同的tomcat之间session要共享，方法就是要定时向其它节点进行广播，其它tomcat发现发现与之不同时便会进行同步，当节点数量较多时，广播将会占用大部分带宽，以至于真正的通信所用的带宽严重不足。因此该架构只能用于节点数小于10的情况。

第二阶段：垂直应用架构

       当访问量逐渐增大，单一应用增加机器带来的加速度越来越小，将应用拆成互不相干的几个应用，以提升效率。举个例子，比如把一个大的项目拆分成订单系统、会员系统、前台系统、后台系统、搜索系统，每个系统自成一家，服务层和web层都在一起，哪个系统压力大，就给那个系统增加节点以提升性能。此时用于加速前端开发的Web框架(MVC)是关键。

第三阶段：分布式服务架构

       当垂直应用越来越多，应用之间交互不可避免，这时代码将非常臃肿（因为同一套代码逻辑可能会被写多遍）。这时将核心业务抽离出来，作为独立的服务，逐渐形成稳定的服务中心，使前端应用能够更快速的响应市场需求。此时用于提高业务复用及整合的分布式服务框架(RPC)是关键。

第四阶段：流动计算架构

       当服务越来越多，容量的评估、小服务资源的浪费等问题逐渐显现，此时需增加一个调度中心基于访问压力实时管理集群容量，提高集群利用率。此时用于提高机器利用率的资源调度和治理中心(SOA)是关键。
## Dubbo的架构

![img](https://img-blog.csdn.net/20170325180636912?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjQ1Mzg0Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

第0步是服务提供者的发布，provider的发布需要用到容器，我们的spring便是专门用来做容器的，因此服务提供者的发布需要用到spring。

第1步是进行注册，就是说服务提供者在发布之后，要在dubbo的注册管理中心进行注册，扮演Registry（注册中心的最好是zookeeper，其次可以选择redis），这样注册中心便知道有哪些服务可供消费者使用了。第2步是消费者要调用服务，但是它是不知道有哪些服务可供调用的，因此它需要先到注册中心进行询问，查询一下是否有自己想要调用的服务，注册中心查找之后发现有匹配的服务可供调用，便会向消费者返回可供调用的服务的IP和端口号。消费者拿到IP和端口号之后，便不再需要经过注册中心，直接就可以访问服务了（这就是第4步），第5步是指dobbo想要监测都是哪些消费者调用了哪些服务，调用了多少次，这样便于管理。



## 版本

zookeeper-3.4.6

cd /usr/local/zookeeper-3.4.6/bin

./zkServer.sh start

./zkServer.sh status

mysql8.0.19(跟mysql5的配置不同)

```properties
#数据库配置
jdbc.driver=com.mysql.cj.jdbc.Driver
#mysql5 com.mysql.jdbc.Driver
jdbc.url=jdbc:mysql://localhost:3306/taotao?characterEncoding=utf8&useSSL=true&serverTimezone=UTC
jdbc.username=root
jdbc.password=123456
```



```properties
#dubbo.properties文件
dubbo.registry.address=zookeeper://127.0.0.1:2181
dubbo.admin.root.password=root#用户名root 密码root
dubbo.admin.guest.password=guest
```

补充知识点：假如我们想知道某个端口比如8080端口被哪个进程占用，以及如何杀掉某个指定进程可以使用如下所示命令进行操作。

```
[root@itcast04 conf]# netstat -apn | grep 8080
tcp        0      0 :::8080                     :::*                        LISTEN      3344/java           
[root@itcast04 conf]# kill -9 3344
```



## redis

### 集群

cd /usr/local/redis-cluster

./start-all.sh

./redis-trib.rb create --replicas 1 192.168.156.60:7001 192.168.156.60:7002 192.168.156.60:7003 192.168.156.60:7004 192.168.156.60:7005 192.168.156.60:7006 

端口7001-7006 redis集群

### 单机版

cd /usr/local/redis/bin

./redis-server redis.conf

查看是否正常启动，使用ps -ef|grep redis或ps aux | grep redis

测试服务

​    使用./redis-cli连接上redis服务

## solr搜索服务器

### 单机版

cd /usr/local/solr/tomcat

bin/startup.sh

bin/shutdown.sh

tailf logs/catalina.out

http://192.168.156.60:9090/solr/#/



### 集群

clientPort=2181-3

server.1=192.168.156.60:2881:3881
server.2=192.168.156.60:2882:3882
server.3=192.168.156.60:2883:3883

cd /usr/local/solr-cloud

./start-zookeeper.sh

查看zookeeper集群启动状态

`[root@itcast solr-cloud]# cd zookeeper01(02,03)/bin`
`[root@itcast bin]# ./zkServer.sh status`

应该有一个leader，两个follower



tomcat集群

修改tomcat端口号---vim tomcat04/conf/server.xml

<Server port="8405" shutdown="SHUTDOWN">

<Connector port="8480" protocol="HTTP/1.1"
               connectionTimeout="20000"
               redirectPort="8443" />

<!-- Define an AJP 1.3 Connector on port 8009 -->
    <Connector port="8409" protocol="AJP/1.3" redirectPort="8443" />

复制单机版solr到集群中

cp -r ../solr/tomcat/webapps/solr/ tomcat01/webapps/ 
cp -r ../solr/tomcat/webapps/solr/ tomcat02/webapps/ 
cp -r ../solr/tomcat/webapps/solr/ tomcat03/webapps/ 
cp -r ../solr/tomcat/webapps/solr/ tomcat04/webapps/

复制solrhome

cp -r ../solr/solrhome/ solrhome01



修改每一个solrhome对应的ip+端口号

[root@itcast solrhome04]# vim solr.xml



建立solrhome与tomcat的联系

 vim tomcat01/webapps/solr/WEB-INF/web.xml

<env-entry>
       <env-entry-name>solr/home</env-entry-name>
       <env-entry-value>/usr/local/solr-cloud/solrhome01</env-entry-value>
       <env-entry-type>java.lang.String</env-entry-type>
    </env-entry>



建立tomcat与zookeeper的联系

1、tomcat的catalinda.sh中的配置：

vim tomcat04/bin/catalina.sh

JAVA_OPTS="-DzkHost=192.168.156.60:2181,192.168.156.60:2182,192.168.156.60:2183"



让zookeeper统一管理配置文件

cd /usr/local/solr-4.10.3/example/scripts/cloud-scripts

./zkcli.sh -zkhost 192.168.156.60:2181,192.168.156.60:2182,192.168.156.60:2183 -cmd upconfig -confdir /usr/local/solr-cloud/solrhome01/collection1/conf/ -confname myconf

cd /usr/local/solr-cloud/zookeeper01/bin

[root@itcast bin]# ./zkCli.sh

./zkCli.sh -server 192.168.156.60:2182

./start-tomcat.sh

http://192.168.156.60:8180/solr/#/

![image-20200518104112183](C:\Users\90466\AppData\Roaming\Typora\typora-user-images\image-20200518104112183.png)

4、SolrCloud创建Collection的命令
http://192.168.156.60:8180/solr/admin/collections?action=CREATE&name=collection2&numShards=2&replicationFactor=2

5、SolrCloud删除Collection的命令
http://192.168.156.60:8180/solr/admin/collections?action=DELETE&name=collection1



## 全局异常处理

![img](https://img-blog.csdn.net/20170503211020321?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjQ1Mzg0Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

```java
//获取logger
private static final Logger logger = LoggerFactory.getLogger(GlobalExceptioResolver.class);
```



## activemq

http://192.168.156.60:8161/admin/

61616--消息服务端口

8161--后台端口

账号密码admin

"Queues"，点对点消息发送界面

"Topics"，发布/订阅模式界面

Send中可以测试发送点对点或发布/订阅两种消息

[root@activemq ~]# cd apache-activemq-5.12.0/bin
[root@activemq bin]# ./activemq start

 topic默认不持久化，消费者必须在线，才能接收到生产者发送的消息，否则将不会保存生产者发送的消息

## Nginx

```xml-dtd
#配置反向代理+负载均衡
#需要修改hosts文件
upstream tomcat1{
	server 192.168.156.60:8082 weight=1;
	server 192.168.156.60:8083 weight=2;
}
 server {
        listen       80;
        server_name  www.mytest1.com;
        location / {
            proxy_pass  http://tomcat1;
            index  index.html index.htm;
        }
    }
upstream tomcat2{
	server 192.168.156.60:8081;
}
 server {
        listen       80;
        server_name  www.mytest2.com;
        location / {
            proxy_pass  http://tomcat2;
            index  index.html index.htm;
        }
    }
```

##  注意事项

.html后缀在SpringMVC中使用@ResponseBody会报406

博文：https://blog.csdn.net/u012453843/category_6970308.html

源码下载：https://gitee.com/chenyp/TaoTaoShangCheng-idea

没有完成tomcat热部署及mycat分片